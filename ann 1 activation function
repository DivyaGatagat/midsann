import numpy as np
import matplotlib.pyplot as plt

# define activation function
# they are mathematical operations applied to output of each nauron in neural network
# functions introduce non linear value to the network allowing it to learn complex patterns and reletionships
# primary purpose is to determine output of nueral network node which becomes the input of the next node

def sigmoid(x):
  return 1/(1+np.exp(-x))
def relu(x):
  return np.maximum(0,x)
def tanh(x):
  return np.tanh(x)
def softmax(x):
  exp_values=np.exp(x-np.max(x,axis=0,keepdims=True))
  return exp_values/np.sum(exp_values,axis=0,keepdims=True)
def linear(x):
    return x

#generate input values
x=np.linspace(-10,15,100)

#compute activation values
sigmoid_values=sigmoid(x)
relu_values=relu(x)
tanh_values=tanh(x)
softmax_values=softmax(np.vstack([x,x+2]))
linear_values = linear(x)

#plot the activation function
plt.figure(figsize=(12,8))

plt.subplot(2,2,1)
plt.plot(x,sigmoid_values, label='sigmoid')
plt.title('Sigmoid function')
plt.legend()

plt.subplot(2,2,2)
plt.plot(x,relu_values, label='relu')
plt.title('relu function')
plt.legend()

plt.subplot(2,2,3)
plt.plot(x,tanh_values, label='tanh')
plt.title('tanh function')
plt.legend()

plt.subplot(2,2,4)
plt.plot(x,softmax_values[0], label='softmax(x)')
plt.plot(x,softmax_values[1], label='softmax(x+2)')
plt.title('Softmax function')
plt.legend()

plt.subplot(2, 3,5)
plt.plot(x, linear_values, label='Linear')
plt.title('Linear Function')
plt.legend()

plt.tight_layout()
plt.show()


